# üõ°Ô∏è Protection contre les Surcharges - Orchestration Quotidienne

## Vue d'ensemble

Le syst√®me d'orchestration quotidienne traite **93 t√¢ches Inbox + 250 t√¢ches totales** chaque jour. Des protections sont en place pour √©viter les surcharges API.

## üìä Volumes de charge actuels

### Situation normale (93 t√¢ches Inbox)

**√âtape 1: Nettoyage Inbox (80s)**
- 10 appels LLM (1 par batch de 10 t√¢ches)
- 93 appels TickTick (1 update par t√¢che)

**√âtape 2: R√©√©quilibrage (20s)**
- 2 appels TickTick (sync)
- ~15 appels TickTick (reschedule)

**Total: ~100s (2 minutes)**
- 10 appels LLM
- 110 appels TickTick
- **66 req/min TickTick** (66% de la limite 100 req/min) ‚úÖ
- **20% quota GROQ** (5 req/min sur 30 RPM) ‚úÖ

### Sc√©narios √† risque

**Inbox tr√®s remplie (200 t√¢ches)**
- 20 appels LLM
- 217 appels TickTick
- **72 req/min** ‚Üí Proche limite mais g√©rable ‚ö†Ô∏è

**R√©organisation massive (100 t√¢ches d√©plac√©es)**
- 195 appels TickTick
- **117 req/min** ‚Üí D√©passement, throttle actif ‚ùå

## üõ°Ô∏è Protections en place

### 1. Throttle automatique TickTick

**Fichier**: `src/api/ticktick-api.js:60-90`

**M√©canisme**: Attente automatique avant chaque requ√™te

```javascript
async waitForRateLimit() {
  // V√©rifier limite 5 minutes (250/300)
  if (this.requestTimestamps.length >= this.maxRequestsPer5Minutes) {
    const waitTime = 300000 - (now - oldestRequest) + 1000;
    await new Promise(resolve => setTimeout(resolve, waitTime));
  }

  // V√©rifier limite 1 minute (80/100)
  if (recentRequests.length >= this.maxRequestsPerMinute) {
    const waitTime = 60000 - (now - oldestRecentRequest) + 1000;
    await new Promise(resolve => setTimeout(resolve, waitTime));
  }
}
```

**Limites configur√©es:**
- 80 req/min (seuil s√©curit√© vs 100 officiel)
- 250 req/5min (seuil s√©curit√© vs 300 officiel)

**Comportement:**
- Si >80 req/min atteint ‚Üí **Pause automatique jusqu'√† 1min suivante**
- Si >250 req/5min atteint ‚Üí **Pause automatique jusqu'√† 5min suivantes**
- **Aucune intervention manuelle requise** ‚úÖ

### 2. D√©lai entre batches Inbox

**Fichier**: `src/llm/intelligent-agent.js:967-970`

**M√©canisme**: Pause de 2 secondes entre chaque batch

```javascript
// Attendre un peu entre les batches pour √©viter rate limit
if (batchIndex < batches.length - 1) {
  await new Promise(resolve => setTimeout(resolve, 2000));
}
```

**Impact:**
- 10 batches √ó 2s = 20 secondes de d√©lai total
- Lisse la charge sur 100s au lieu de 80s
- R√©duit le pic de charge TickTick

**Calcul sans d√©lai:**
- 93 updates en 80s = 70 req/min ‚ùå (proche limite)

**Calcul avec d√©lai:**
- 93 updates en 100s = **56 req/min** ‚úÖ (marge confortable)

### 3. Fallback LLM intelligent

**Fichier**: `src/llm/intelligent-agent.js:81-108`

**M√©canisme**: Bascule automatique GROQ ‚Üí Gemini

```javascript
// Essayer GROQ en priorit√©
if (this.groq) {
  try {
    const completion = await this.groq.chat.completions.create(...);
  } catch (error) {
    if (error.message.includes('rate_limit')) {
      logger.warn('‚ö†Ô∏è  GROQ rate limit, bascule sur Gemini...');
    }
  }
}

// Fallback Gemini si GROQ √©choue
if (this.gemini) {
  const result = await chat.sendMessage(...);
}
```

**Limites LLM:**
- **GROQ**: 30 req/min, 14400 req/jour (gratuit)
- **Gemini**: 15 req/min, 1500 req/jour (gratuit)

**Utilisation actuelle:**
- GROQ: 10 req en 2min = **5 req/min** (17% du quota) ‚úÖ
- Gemini (si fallback): 10 req en 2min = **5 req/min** (33% du quota) ‚úÖ

### 4. Cache 2 minutes

**Fichier**: `src/api/ticktick-api.js:92-110`

**M√©canisme**: Cache en m√©moire pour lectures r√©p√©t√©es

```javascript
getFromCache(key) {
  const cached = this.cache.get(key);
  if (cached && Date.now() - cached.timestamp < this.cacheTTL) {
    return cached.data; // √âvite appel API
  }
  return null;
}
```

**TTL**: 2 minutes (120000ms)

**B√©n√©fices:**
- √âvite duplicatas lors de lectures rapproch√©es
- R√©duit charge sur getProjects(), getTasks()
- **Pas de cache sur writes** (create/update/delete) ‚Üí Coh√©rence garantie

### 5. Batch processing

**Fichier**: `src/llm/intelligent-agent.js:850-854`

**M√©canisme**: Divise Inbox en lots de 10 t√¢ches

```javascript
const BATCH_SIZE = 10;
const batches = [];
for (let i = 0; i < inboxTasks.length; i += BATCH_SIZE) {
  batches.push(inboxTasks.slice(i, i + BATCH_SIZE));
}
```

**Avantages:**
- **√âvite timeout LLM** (10 t√¢ches = ~8s de processing)
- **Lisse la charge** (10 batches √ó 8s = 80s au lieu d'1 gros appel)
- **Partial success** (si batch 3 √©choue, batches 1-2 sont d√©j√† trait√©s)

**Alternative sans batch:**
- 1 appel LLM avec 93 t√¢ches = **Timeout apr√®s 30s** ‚ùå
- Tout √©choue si 1 seule erreur ‚ùå

### 6. Retry automatique

**Fichier**: `src/api/ticktick-api.js:46-57`

**M√©canisme**: Intercepteur axios pour retry sur 401

```javascript
this.client.interceptors.response.use(
  (response) => response,
  async (error) => {
    if (error.response?.status === 401 && this.refreshToken) {
      await this.refreshAccessToken();
      return this.client.request(error.config); // Retry
    }
    return Promise.reject(error);
  }
);
```

**Cas g√©r√©s:**
- Token expir√© ‚Üí Refresh auto + retry
- Rate limit ‚Üí G√©r√© par throttle (pas de retry imm√©diat)

## üìà Monitoring en temps r√©el

### Logs throttle

Quand le syst√®me approche des limites, tu verras:

```
‚ö†Ô∏è  Rate limit 1min approch√© (78/80), attente 15s
```

```
‚ö†Ô∏è  Rate limit 5min approch√© (248/250), attente 45s
```

**Interpr√©tation:**
- C'est **NORMAL** dans les cas de charge √©lev√©e
- Le syst√®me **s'auto-r√©gule** automatiquement
- Pas d'intervention n√©cessaire ‚úÖ

### Logs batch processing

```
üì¶ Traitement batch 1/10 (10 t√¢ches)
ü§ñ R√©ponse LLM batch 1: ...
‚úÖ 8/10 t√¢ches batch 1 d√©plac√©es
[Pause 2s]
üì¶ Traitement batch 2/10 (10 t√¢ches)
```

### Dashboard ActivityTracker

Progression visible en temps r√©el:

```
üéØ Orchestration Quotidienne Compl√®te
  ‚îî‚îÄ üì• Nettoyage Inbox: 35/93 t√¢ches (batch 4/10)
  ‚îî‚îÄ üîÑ R√©√©quilibrage: En attente...
  ‚îî‚îÄ ‚è±Ô∏è Dur√©e: 38s
```

## üéØ Recommandations

### ‚úÖ Situation actuelle (93 t√¢ches)

**Rien √† faire** - Toutes les protections sont en place et suffisantes.

- Charge: 66% ‚Üí Marge 34%
- Throttle: Activ√©
- D√©lai batches: 2s
- Fallback LLM: Op√©rationnel

### ‚ö†Ô∏è Si Inbox monte √† 150-200 t√¢ches

**Surveillance accrue** - Le syst√®me g√©rera mais sera plus lent

- Dur√©e: 2min ‚Üí 4-5min (throttle actif)
- Charge: 70-80% ‚Üí Marge 20-30%
- **Action**: Aucune (throttle g√®re automatiquement)

### üî¥ Si Inbox monte √† 300+ t√¢ches

**Optimisation recommand√©e** - R√©duire taille batch

Modifier `src/llm/intelligent-agent.js:850`:
```javascript
const BATCH_SIZE = 5; // Au lieu de 10
```

**Impact:**
- Plus de batches mais plus petits
- Charge plus liss√©e (60 batches √ó 4s = 240s)
- Rate: 300 updates / 240s = **75 req/min** ‚úÖ

**D√©lai augment√©:**
```javascript
await new Promise(resolve => setTimeout(resolve, 3000)); // 3s au lieu de 2s
```

## üîç Comment tester les protections

### Test 1: Throttle TickTick

Simuler charge √©lev√©e:
```bash
node /tmp/test-throttle-ticktick.js
```

Observer dans les logs:
```
‚ö†Ô∏è  Rate limit 1min approch√© (80/80), attente 12s
```

### Test 2: Fallback LLM

Atteindre rate limit GROQ (30 req/min):
```bash
# Lancer 30 appels LLM en 1 minute
for i in {1..30}; do
  node /tmp/test-llm-call.js &
done
```

Observer dans les logs:
```
‚ö†Ô∏è  GROQ rate limit atteint, bascule sur Gemini...
```

### Test 3: Orchestration compl√®te

Lancer en prod:
```bash
node scripts/daily-inbox-cleanup.js
```

Observer dur√©e r√©elle vs estim√©e:
- **Estim√©**: 100s (2min)
- **R√©el avec throttle l√©ger**: 110-120s
- **R√©el avec throttle actif**: 180-240s

## üìä M√©triques de sant√©

### Valeurs normales

- **Dur√©e orchestration**: 90-120s
- **Throttle warnings**: 0-2 pendant ex√©cution
- **Batches r√©ussis**: 8-10/10 (80-100%)
- **T√¢ches d√©plac√©es**: 70-90/93 (75-97%)

### Valeurs pr√©occupantes

- **Dur√©e orchestration**: >180s (throttle tr√®s actif)
- **Throttle warnings**: >5 (surcharge)
- **Batches r√©ussis**: <6/10 (<60%)
- **T√¢ches d√©plac√©es**: <50/93 (<50%)

**Action si valeurs pr√©occupantes:**
1. V√©rifier Inbox (trop pleine?)
2. V√©rifier quotas LLM (√©puis√©s?)
3. R√©duire BATCH_SIZE (10 ‚Üí 5)
4. Augmenter d√©lai batches (2s ‚Üí 3s)

## üö® Cas d'urgence

### Rate limit 429 TickTick malgr√© throttle

**Sympt√¥me:**
```
Error: Request failed with status code 429
```

**Cause:** Autres scripts/outils utilisent l'API en parall√®le

**Solution imm√©diate:**
1. Arr√™ter autres scripts TickTick
2. Attendre 5 minutes compl√®tes
3. Relancer orchestration

**Solution permanente:**
R√©duire seuils throttle dans `src/api/ticktick-api.js:19-20`:
```javascript
this.maxRequestsPerMinute = 60; // Au lieu de 80
this.maxRequestsPer5Minutes = 200; // Au lieu de 250
```

### LLM inaccessible (GROQ + Gemini down)

**Sympt√¥me:**
```
‚ùå Erreur nettoyage Inbox: Both GROQ and Gemini unavailable
```

**Solution imm√©diate:**
Nettoyage Inbox √©choue mais r√©√©quilibrage continue (pas de d√©pendance LLM)

**Solution permanente:**
Ajouter 3√®me fallback (OpenRouter, Anthropic, etc.)

## ‚úÖ Conclusion

**Le syst√®me actuel est ROBUSTE et PROT√âG√â:**

1. ‚úÖ **Throttle automatique** emp√™che d√©passement rate limits
2. ‚úÖ **D√©lai entre batches** lisse la charge
3. ‚úÖ **Fallback LLM** assure continuit√© service
4. ‚úÖ **Cache intelligent** r√©duit appels inutiles
5. ‚úÖ **Batch processing** √©vite timeouts
6. ‚úÖ **Retry automatique** g√®re erreurs transitoires

**Avec 93 t√¢ches Inbox actuelles:**
- Utilisation: **66% TickTick, 20% LLM**
- Marge s√©curit√©: **34% TickTick, 80% LLM**
- Dur√©e: **100 secondes (2 minutes)**

**Aucune action requise** - Le syst√®me peut g√©rer jusqu'√† 150-200 t√¢ches Inbox sans intervention.

---

**Derni√®re mise √† jour**: 16 octobre 2025
**Version**: 1.0.0
**Status**: ‚úÖ Op√©rationnel et prot√©g√©
